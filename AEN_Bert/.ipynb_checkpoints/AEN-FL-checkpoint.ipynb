{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import numpy as np\n",
    "data_process=imp.load_source('data_process','../data_process.py')\n",
    "from data_process import get_xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents,train_contexts,train_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Train.xml')\n",
    "validation_sents,validation_contexts,validation_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Dev.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name='hfl/chinese-bert-wwm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(bert_name, return_dict=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertTensor(text_list,MAX_LEN = 128):\n",
    "    words_idx = []\n",
    "    for sent in text_list:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = MAX_LEN,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        \n",
    "        words_idx.append(encoded_sent)\n",
    "    \n",
    "    words_idx=pad_sequences(words_idx, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    words_masks=[]\n",
    "    for sent in words_idx:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        words_masks.append(att_mask)\n",
    "        \n",
    "    return torch.tensor(words_idx),torch.tensor(words_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "train_sents,train_sents_masks=get_bertTensor(train_sents)\n",
    "train_contexts,train_contexts_masks=get_bertTensor(train_contexts,MAX_LEN=256)\n",
    "train_labels=torch.tensor(train_labels)\n",
    "\n",
    "validation_sents,validation_sents_masks=get_bertTensor(validation_sents)\n",
    "validation_contexts,validation_contexts_masks=get_bertTensor(validation_contexts,MAX_LEN=256)\n",
    "validation_labels=torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "train_data = TensorDataset(train_sents,train_sents_masks,train_contexts,train_contexts_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_sents,validation_sents_masks,validation_contexts,\n",
    "                                validation_contexts_masks,validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 AEN_Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "from layers.squeeze_embedding import SqueezeEmbedding\n",
    "from layers.attention import Attention, NoQueryAttention\n",
    "from layers.point_wise_feed_forward import PositionwiseFeedForward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEN_BERT(nn.Module):\n",
    "    def __init__(self,bert):\n",
    "        super(AEN_BERT, self).__init__()\n",
    "\n",
    "        dropout=0.1\n",
    "        bert_dim=768    \n",
    "        hidden_dim=300\n",
    "        polarities_dim=3\n",
    "        \n",
    "        self.drop_path_prob=0.0\n",
    "        self.bert=bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_k = Attention(bert_dim, out_dim=hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.attn_q = Attention(bert_dim, out_dim=hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.ffn_c = PositionwiseFeedForward(hidden_dim, dropout=dropout)\n",
    "        self.ffn_t = PositionwiseFeedForward(hidden_dim, dropout=dropout)\n",
    "\n",
    "        self.attn_s1 = Attention(hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.dense = nn.Linear(hidden_dim*3, polarities_dim)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "#         context, target = inputs[0], inputs[1]\n",
    "        context_len=128\n",
    "        target_len=128\n",
    "        \n",
    "        target,_=bert(a_input_ids, token_type_ids=None, attention_mask=a_input_mask,)\n",
    "        context,_=bert(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,)\n",
    "#         context_len = torch.sum(context != 0, dim=-1)\n",
    "#         target_len = torch.sum(target != 0, dim=-1)\n",
    "#         context = self.squeeze_embedding(context, context_len)\n",
    "#         context, _ = self.bert(context)\n",
    "        context = self.dropout(context)\n",
    "        target = self.dropout(target)\n",
    "\n",
    "        hc, _ = self.attn_k(context, context)\n",
    "        hc = self.ffn_c(hc)\n",
    "#         ht, _ = self.attn_q(context, target)\n",
    "        ht, _ = self.attn_q(target, target)\n",
    "        ht = self.ffn_t(ht)\n",
    "\n",
    "        s1, _ = self.attn_s1(hc, ht)\n",
    "\n",
    "        hc_mean = torch.div(torch.sum(hc, dim=1), context_len)\n",
    "        ht_mean = torch.div(torch.sum(ht, dim=1), target_len)\n",
    "        s1_mean = torch.div(torch.sum(s1, dim=1), context_len)\n",
    "\n",
    "        x = torch.cat((hc_mean, s1_mean, ht_mean), dim=-1)\n",
    "        out=self.dense(x)\n",
    "#         out = torch.argmax(self.softmax(self.dense(x)),dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "from focalloss import FocalLoss\n",
    "from sklearn.metrics import classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0,2,3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda\")#select gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert.cuda()\n",
    "model=AEN_BERT(bert)\n",
    "model.cuda()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "alpha=torch.tensor([[2.],[1.],[1.]])\n",
    "criterion=FocalLoss(3,alpha,gamma=2)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot,acc_plot=[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/xuke/temp/Graduation_design/AEN_Bert/focalloss.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  P = F.softmax(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  2,463.    Elapsed: 0:00:12.\n",
      "loss: 0.3313, acc: 0.5610\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:24.\n",
      "loss: 0.2537, acc: 0.7125\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:36.\n",
      "loss: 0.2339, acc: 0.6958\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:48.\n",
      "loss: 0.2064, acc: 0.7500\n",
      "  Batch   200  of  2,463.    Elapsed: 0:01:00.\n",
      "loss: 0.1823, acc: 0.7417\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:12.\n",
      "loss: 0.2195, acc: 0.7125\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:24.\n",
      "loss: 0.1934, acc: 0.7333\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:36.\n",
      "loss: 0.1957, acc: 0.7333\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:48.\n",
      "loss: 0.1735, acc: 0.7667\n",
      "  Batch   400  of  2,463.    Elapsed: 0:02:00.\n",
      "loss: 0.1772, acc: 0.7708\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:13.\n",
      "loss: 0.1771, acc: 0.7958\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:25.\n",
      "loss: 0.1646, acc: 0.7792\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:37.\n",
      "loss: 0.1914, acc: 0.7333\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:49.\n",
      "loss: 0.1452, acc: 0.8000\n",
      "  Batch   600  of  2,463.    Elapsed: 0:03:01.\n",
      "loss: 0.1911, acc: 0.7333\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:14.\n",
      "loss: 0.1733, acc: 0.7583\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:26.\n",
      "loss: 0.1659, acc: 0.7542\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:38.\n",
      "loss: 0.1642, acc: 0.7833\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:50.\n",
      "loss: 0.1552, acc: 0.8083\n",
      "  Batch   800  of  2,463.    Elapsed: 0:04:02.\n",
      "loss: 0.1637, acc: 0.7458\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:14.\n",
      "loss: 0.1491, acc: 0.7792\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:26.\n",
      "loss: 0.1771, acc: 0.7333\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:39.\n",
      "loss: 0.1430, acc: 0.8042\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:52.\n",
      "loss: 0.1823, acc: 0.7917\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:05:05.\n",
      "loss: 0.1717, acc: 0.8000\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:18.\n",
      "loss: 0.1383, acc: 0.8417\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:30.\n",
      "loss: 0.1571, acc: 0.7792\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:43.\n",
      "loss: 0.1352, acc: 0.7917\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:55.\n",
      "loss: 0.1447, acc: 0.8000\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:06:07.\n",
      "loss: 0.1588, acc: 0.7667\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:19.\n",
      "loss: 0.1498, acc: 0.7833\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:31.\n",
      "loss: 0.1374, acc: 0.7917\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:44.\n",
      "loss: 0.1573, acc: 0.7458\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:57.\n",
      "loss: 0.1590, acc: 0.7708\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:07:09.\n",
      "loss: 0.1222, acc: 0.8125\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:22.\n",
      "loss: 0.1680, acc: 0.7667\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:34.\n",
      "loss: 0.1669, acc: 0.7625\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:46.\n",
      "loss: 0.1359, acc: 0.7792\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:07:58.\n",
      "loss: 0.1484, acc: 0.8083\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:08:11.\n",
      "loss: 0.1466, acc: 0.8000\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:23.\n",
      "loss: 0.1256, acc: 0.8292\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:35.\n",
      "loss: 0.1499, acc: 0.7583\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:47.\n",
      "loss: 0.1643, acc: 0.7625\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:09:00.\n",
      "loss: 0.1718, acc: 0.7375\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:09:13.\n",
      "loss: 0.1359, acc: 0.7917\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:25.\n",
      "loss: 0.1176, acc: 0.8375\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:37.\n",
      "loss: 0.1377, acc: 0.7750\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:49.\n",
      "loss: 0.1374, acc: 0.8375\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:10:01.\n",
      "loss: 0.1727, acc: 0.7250\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:10:14.\n",
      "loss: 0.1189, acc: 0.8458\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:10:26.\n",
      "loss: 0.1521, acc: 0.7875\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:38.\n",
      "loss: 0.1319, acc: 0.8125\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:50.\n",
      "loss: 0.1672, acc: 0.7125\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:11:02.\n",
      "loss: 0.1211, acc: 0.8292\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:11:14.\n",
      "loss: 0.1674, acc: 0.7208\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:11:27.\n",
      "loss: 0.1476, acc: 0.7750\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:40.\n",
      "loss: 0.1648, acc: 0.7792\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:53.\n",
      "loss: 0.1443, acc: 0.7792\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:12:07.\n",
      "loss: 0.1125, acc: 0.8167\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:12:21.\n",
      "loss: 0.1616, acc: 0.7500\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:12:34.\n",
      "loss: 0.1737, acc: 0.7708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      2553\n",
      "           1       0.55      0.86      0.67      1232\n",
      "           2       0.87      0.48      0.62      1358\n",
      "\n",
      "    accuracy                           0.76      5143\n",
      "   macro avg       0.77      0.73      0.72      5143\n",
      "weighted avg       0.81      0.76      0.76      5143\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,463.    Elapsed: 0:00:13.\n",
      "loss: 0.1151, acc: 0.8089\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:25.\n",
      "loss: 0.1102, acc: 0.8500\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:37.\n",
      "loss: 0.0687, acc: 0.8917\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:49.\n",
      "loss: 0.0655, acc: 0.8958\n",
      "  Batch   200  of  2,463.    Elapsed: 0:01:02.\n",
      "loss: 0.0868, acc: 0.8833\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:14.\n",
      "loss: 0.1079, acc: 0.8292\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:26.\n",
      "loss: 0.1101, acc: 0.8583\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:38.\n",
      "loss: 0.0882, acc: 0.8417\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:51.\n",
      "loss: 0.0959, acc: 0.8208\n",
      "  Batch   400  of  2,463.    Elapsed: 0:02:03.\n",
      "loss: 0.0980, acc: 0.8792\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:15.\n",
      "loss: 0.1132, acc: 0.8000\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:28.\n",
      "loss: 0.1144, acc: 0.8542\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:41.\n",
      "loss: 0.0950, acc: 0.8333\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:53.\n",
      "loss: 0.0714, acc: 0.8875\n",
      "  Batch   600  of  2,463.    Elapsed: 0:03:05.\n",
      "loss: 0.0793, acc: 0.8750\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:17.\n",
      "loss: 0.1122, acc: 0.8333\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:29.\n",
      "loss: 0.0995, acc: 0.8500\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:41.\n",
      "loss: 0.1020, acc: 0.8583\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:54.\n",
      "loss: 0.0766, acc: 0.8833\n",
      "  Batch   800  of  2,463.    Elapsed: 0:04:06.\n",
      "loss: 0.0913, acc: 0.8583\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:18.\n",
      "loss: 0.1069, acc: 0.8542\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:30.\n",
      "loss: 0.0928, acc: 0.8708\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:42.\n",
      "loss: 0.0926, acc: 0.8292\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:54.\n",
      "loss: 0.1005, acc: 0.8375\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:05:08.\n",
      "loss: 0.0859, acc: 0.8583\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:21.\n",
      "loss: 0.1066, acc: 0.8500\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:33.\n",
      "loss: 0.1196, acc: 0.8208\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:45.\n",
      "loss: 0.0988, acc: 0.8667\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:58.\n",
      "loss: 0.0687, acc: 0.9042\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:06:10.\n",
      "loss: 0.0746, acc: 0.8667\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:22.\n",
      "loss: 0.1172, acc: 0.8375\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:35.\n",
      "loss: 0.1095, acc: 0.8125\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:47.\n",
      "loss: 0.1060, acc: 0.7875\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:59.\n",
      "loss: 0.0885, acc: 0.8458\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:07:12.\n",
      "loss: 0.0942, acc: 0.8292\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:25.\n",
      "loss: 0.1085, acc: 0.8542\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:38.\n",
      "loss: 0.1101, acc: 0.8083\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:50.\n",
      "loss: 0.1302, acc: 0.8125\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:08:02.\n",
      "loss: 0.0852, acc: 0.8208\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:08:15.\n",
      "loss: 0.1172, acc: 0.8625\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:27.\n",
      "loss: 0.0867, acc: 0.8792\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:39.\n",
      "loss: 0.1043, acc: 0.8667\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:52.\n",
      "loss: 0.0883, acc: 0.8625\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:09:04.\n",
      "loss: 0.1007, acc: 0.8458\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:09:16.\n",
      "loss: 0.0916, acc: 0.8542\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:29.\n",
      "loss: 0.1003, acc: 0.7875\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:41.\n",
      "loss: 0.0690, acc: 0.8625\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:53.\n",
      "loss: 0.1024, acc: 0.8458\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:10:05.\n",
      "loss: 0.1137, acc: 0.8167\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:10:18.\n",
      "loss: 0.0759, acc: 0.8792\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:10:30.\n",
      "loss: 0.0959, acc: 0.8542\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:43.\n",
      "loss: 0.0839, acc: 0.8542\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:55.\n",
      "loss: 0.0872, acc: 0.8625\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:11:07.\n",
      "loss: 0.0859, acc: 0.8292\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:11:19.\n",
      "loss: 0.1238, acc: 0.8292\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:11:32.\n",
      "loss: 0.1250, acc: 0.8042\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:44.\n",
      "loss: 0.0961, acc: 0.8375\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:56.\n",
      "loss: 0.0880, acc: 0.8750\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:12:10.\n",
      "loss: 0.1265, acc: 0.8250\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:12:24.\n",
      "loss: 0.0750, acc: 0.8708\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:12:38.\n",
      "loss: 0.1100, acc: 0.8333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85      2553\n",
      "           1       0.62      0.81      0.70      1232\n",
      "           2       0.78      0.81      0.79      1358\n",
      "\n",
      "    accuracy                           0.80      5143\n",
      "   macro avg       0.78      0.80      0.78      5143\n",
      "weighted avg       0.82      0.80      0.80      5143\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,463.    Elapsed: 0:00:13.\n",
      "loss: 0.0575, acc: 0.9024\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:25.\n",
      "loss: 0.0410, acc: 0.9208\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:37.\n",
      "loss: 0.0566, acc: 0.8792\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:49.\n",
      "loss: 0.0498, acc: 0.9375\n",
      "  Batch   200  of  2,463.    Elapsed: 0:01:02.\n",
      "loss: 0.0507, acc: 0.9292\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:14.\n",
      "loss: 0.0586, acc: 0.9375\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:27.\n",
      "loss: 0.0480, acc: 0.9292\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:39.\n",
      "loss: 0.0624, acc: 0.8958\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:51.\n",
      "loss: 0.0583, acc: 0.8875\n",
      "  Batch   400  of  2,463.    Elapsed: 0:02:03.\n",
      "loss: 0.0603, acc: 0.9167\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:16.\n",
      "loss: 0.0612, acc: 0.9125\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:28.\n",
      "loss: 0.0528, acc: 0.9000\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:40.\n",
      "loss: 0.0578, acc: 0.9125\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:53.\n",
      "loss: 0.0469, acc: 0.9250\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "epochs=4\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "#     total_loss=0\n",
    "    model.train()\n",
    "    n_correct, n_total, loss_total = 0, 0, 0\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        a_input_ids  = batch[0].to(device)\n",
    "        a_input_mask = batch[1].to(device)\n",
    "        b_input_ids  = batch[2].to(device)\n",
    "        b_input_mask = batch[3].to(device)\n",
    "        labels       = batch[4].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs=a_input_ids,a_input_mask,b_input_ids,b_input_mask \n",
    "        predict=model(inputs)\n",
    "        loss=criterion(predict,labels)\n",
    "#         print(predict,labels,loss)\n",
    "#         total_loss+=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_correct += (torch.argmax(predict, -1) == labels).sum().item()\n",
    "        n_total += len(predict)\n",
    "        loss_total += loss.item() * len(predict)\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))        \n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "            loss_plot.append(train_loss)\n",
    "            acc_plot.append(train_acc)\n",
    "            logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))   \n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            \n",
    "    true_labels,predict_labels=[],[]        \n",
    "    for batch in validation_dataloader:\n",
    "        a_input_ids  = batch[0].to(device)\n",
    "        a_input_mask = batch[1].to(device)\n",
    "        b_input_ids  = batch[2].to(device)\n",
    "        b_input_mask = batch[3].to(device)\n",
    "        labels       = batch[4]\n",
    "        inputs=a_input_ids,a_input_mask,b_input_ids,b_input_mask\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(inputs)        \n",
    "        predict=outputs.detach().cpu().numpy()\n",
    "        predict=np.argmax(predict, axis=1).flatten()\n",
    "        predict_labels.append(predict)\n",
    "        true_labels.append(labels)\n",
    "    true_labels=[y for x in true_labels for y in x]\n",
    "    predict_labels=[y for x in predict_labels for y in x]\n",
    "#     f1_score=f1_score(true_labels,predict_labels,average='weighted')\n",
    "    print(classification_report(true_labels,predict_labels))\n",
    "#     print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Training loss and accuracy\") \n",
    "plt.xlabel(\"train epoch\") \n",
    "plt.ylabel(\"acc or loss\") \n",
    "plt.plot(loss_plot, color='firebrick',   label='training loss')\n",
    "plt.plot(acc_plot,  color='darkorange',   label='accuracy')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
