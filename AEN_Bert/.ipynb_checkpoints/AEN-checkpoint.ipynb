{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import numpy as np\n",
    "data_process=imp.load_source('data_process','../data_process.py')\n",
    "from data_process import get_xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents,train_contexts,train_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Train.xml')\n",
    "validation_sents,validation_contexts,validation_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Dev.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name='hfl/chinese-bert-wwm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(bert_name, return_dict=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertTensor(text_list,MAX_LEN = 128):\n",
    "    words_idx = []\n",
    "    for sent in text_list:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = MAX_LEN,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        \n",
    "        words_idx.append(encoded_sent)\n",
    "    \n",
    "    words_idx=pad_sequences(words_idx, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    words_masks=[]\n",
    "    for sent in words_idx:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        words_masks.append(att_mask)\n",
    "        \n",
    "    return torch.tensor(words_idx),torch.tensor(words_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "train_sents,train_sents_masks=get_bertTensor(train_sents)\n",
    "train_contexts,train_contexts_masks=get_bertTensor(train_contexts,MAX_LEN=256)\n",
    "train_labels=torch.tensor(train_labels)\n",
    "\n",
    "validation_sents,validation_sents_masks=get_bertTensor(validation_sents)\n",
    "validation_contexts,validation_contexts_masks=get_bertTensor(validation_contexts,MAX_LEN=256)\n",
    "validation_labels=torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "train_data = TensorDataset(train_sents,train_sents_masks,train_contexts,train_contexts_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_sents,validation_sents_masks,validation_contexts,\n",
    "                                validation_contexts_masks,validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 AEN_Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "from layers.squeeze_embedding import SqueezeEmbedding\n",
    "from layers.attention import Attention, NoQueryAttention\n",
    "from layers.point_wise_feed_forward import PositionwiseFeedForward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEN_BERT(nn.Module):\n",
    "    def __init__(self,bert):\n",
    "        super(AEN_BERT, self).__init__()\n",
    "\n",
    "        dropout=0.1\n",
    "        bert_dim=768    \n",
    "        hidden_dim=300\n",
    "        polarities_dim=3\n",
    "        \n",
    "        self.drop_path_prob=0.0\n",
    "        self.bert=bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_k = Attention(bert_dim, out_dim=hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.attn_q = Attention(bert_dim, out_dim=hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.ffn_c = PositionwiseFeedForward(hidden_dim, dropout=dropout)\n",
    "        self.ffn_t = PositionwiseFeedForward(hidden_dim, dropout=dropout)\n",
    "\n",
    "        self.attn_s1 = Attention(hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.dense = nn.Linear(hidden_dim*3, polarities_dim)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "#         context, target = inputs[0], inputs[1]\n",
    "        context_len=128\n",
    "        target_len=128\n",
    "        \n",
    "        target,_=bert(a_input_ids, token_type_ids=None, attention_mask=a_input_mask,)\n",
    "        context,_=bert(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,)\n",
    "#         context_len = torch.sum(context != 0, dim=-1)\n",
    "#         target_len = torch.sum(target != 0, dim=-1)\n",
    "#         context = self.squeeze_embedding(context, context_len)\n",
    "#         context, _ = self.bert(context)\n",
    "        context = self.dropout(context)\n",
    "        target = self.dropout(target)\n",
    "\n",
    "        hc, _ = self.attn_k(context, context)\n",
    "        hc = self.ffn_c(hc)\n",
    "#         ht, _ = self.attn_q(context, target)\n",
    "        ht, _ = self.attn_q(target, target)\n",
    "        ht = self.ffn_t(ht)\n",
    "\n",
    "        s1, _ = self.attn_s1(hc, ht)\n",
    "\n",
    "        hc_mean = torch.div(torch.sum(hc, dim=1), context_len)\n",
    "        ht_mean = torch.div(torch.sum(ht, dim=1), target_len)\n",
    "        s1_mean = torch.div(torch.sum(s1, dim=1), context_len)\n",
    "\n",
    "        x = torch.cat((hc_mean, s1_mean, ht_mean), dim=-1)\n",
    "        out=self.dense(x)\n",
    "#         out = torch.argmax(self.softmax(self.dense(x)),dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0,2,3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda:0\")#select gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert.cuda()\n",
    "model=AEN_BERT(bert)\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot,acc_plot=[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  2,463.    Elapsed: 0:00:12.\n",
      "loss: 0.9594, acc: 0.5650\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:23.\n",
      "loss: 0.7939, acc: 0.6875\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:35.\n",
      "loss: 0.7350, acc: 0.6792\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:47.\n",
      "loss: 0.7322, acc: 0.6958\n",
      "  Batch   200  of  2,463.    Elapsed: 0:00:58.\n",
      "loss: 0.6682, acc: 0.7208\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:10.\n",
      "loss: 0.6052, acc: 0.7500\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:21.\n",
      "loss: 0.5760, acc: 0.7750\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:33.\n",
      "loss: 0.6680, acc: 0.7042\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:45.\n",
      "loss: 0.5416, acc: 0.7667\n",
      "  Batch   400  of  2,463.    Elapsed: 0:01:57.\n",
      "loss: 0.5287, acc: 0.7958\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:08.\n",
      "loss: 0.5495, acc: 0.7875\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:20.\n",
      "loss: 0.5062, acc: 0.8167\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:32.\n",
      "loss: 0.5451, acc: 0.7625\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:44.\n",
      "loss: 0.6281, acc: 0.7208\n",
      "  Batch   600  of  2,463.    Elapsed: 0:02:55.\n",
      "loss: 0.5755, acc: 0.7625\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:07.\n",
      "loss: 0.5454, acc: 0.7708\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:19.\n",
      "loss: 0.5072, acc: 0.7917\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:31.\n",
      "loss: 0.4522, acc: 0.8208\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:43.\n",
      "loss: 0.5850, acc: 0.7667\n",
      "  Batch   800  of  2,463.    Elapsed: 0:03:54.\n",
      "loss: 0.4445, acc: 0.8292\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:06.\n",
      "loss: 0.5284, acc: 0.8042\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:18.\n",
      "loss: 0.7033, acc: 0.7042\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:30.\n",
      "loss: 0.5460, acc: 0.7792\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:41.\n",
      "loss: 0.5546, acc: 0.7417\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:04:53.\n",
      "loss: 0.5580, acc: 0.7792\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:05.\n",
      "loss: 0.4135, acc: 0.8458\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:17.\n",
      "loss: 0.5551, acc: 0.7792\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:28.\n",
      "loss: 0.4681, acc: 0.8208\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:40.\n",
      "loss: 0.4381, acc: 0.8500\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:05:52.\n",
      "loss: 0.4947, acc: 0.8000\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:04.\n",
      "loss: 0.4827, acc: 0.8208\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:15.\n",
      "loss: 0.4717, acc: 0.8208\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:27.\n",
      "loss: 0.4520, acc: 0.8167\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:39.\n",
      "loss: 0.5944, acc: 0.7500\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:06:51.\n",
      "loss: 0.5003, acc: 0.7875\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:02.\n",
      "loss: 0.5304, acc: 0.7833\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:14.\n",
      "loss: 0.4109, acc: 0.8458\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:26.\n",
      "loss: 0.4376, acc: 0.8458\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:07:38.\n",
      "loss: 0.5386, acc: 0.7958\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:07:49.\n",
      "loss: 0.4554, acc: 0.8208\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:01.\n",
      "loss: 0.5129, acc: 0.8042\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:13.\n",
      "loss: 0.4690, acc: 0.8000\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:25.\n",
      "loss: 0.3729, acc: 0.8542\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:08:37.\n",
      "loss: 0.5506, acc: 0.7875\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:08:48.\n",
      "loss: 0.4430, acc: 0.8083\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:00.\n",
      "loss: 0.4143, acc: 0.8333\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:12.\n",
      "loss: 0.4949, acc: 0.8000\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:24.\n",
      "loss: 0.3918, acc: 0.8542\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:09:35.\n",
      "loss: 0.5203, acc: 0.8042\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:09:47.\n",
      "loss: 0.4355, acc: 0.8250\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:09:59.\n",
      "loss: 0.5199, acc: 0.8125\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:11.\n",
      "loss: 0.4533, acc: 0.8292\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:22.\n",
      "loss: 0.3443, acc: 0.8625\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:10:34.\n",
      "loss: 0.4900, acc: 0.8167\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:10:46.\n",
      "loss: 0.3928, acc: 0.8417\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:10:58.\n",
      "loss: 0.3975, acc: 0.8542\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:09.\n",
      "loss: 0.4925, acc: 0.7833\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:21.\n",
      "loss: 0.4569, acc: 0.7792\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:11:33.\n",
      "loss: 0.4500, acc: 0.8125\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:11:45.\n",
      "loss: 0.4916, acc: 0.7958\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:11:56.\n",
      "loss: 0.4535, acc: 0.8250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      2553\n",
      "           1       0.68      0.77      0.72      1232\n",
      "           2       0.87      0.70      0.78      1358\n",
      "\n",
      "    accuracy                           0.82      5143\n",
      "   macro avg       0.81      0.79      0.80      5143\n",
      "weighted avg       0.83      0.82      0.82      5143\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,463.    Elapsed: 0:00:12.\n",
      "loss: 0.3055, acc: 0.8780\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:24.\n",
      "loss: 0.3628, acc: 0.8792\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:36.\n",
      "loss: 0.3112, acc: 0.8708\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:47.\n",
      "loss: 0.3425, acc: 0.8750\n",
      "  Batch   200  of  2,463.    Elapsed: 0:00:59.\n",
      "loss: 0.2967, acc: 0.8792\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:11.\n",
      "loss: 0.2903, acc: 0.8833\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:23.\n",
      "loss: 0.2771, acc: 0.8708\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:35.\n",
      "loss: 0.2734, acc: 0.8958\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:46.\n",
      "loss: 0.2891, acc: 0.8792\n",
      "  Batch   400  of  2,463.    Elapsed: 0:01:58.\n",
      "loss: 0.3234, acc: 0.8917\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:10.\n",
      "loss: 0.3168, acc: 0.8708\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:22.\n",
      "loss: 0.3758, acc: 0.8500\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:34.\n",
      "loss: 0.2845, acc: 0.9000\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:46.\n",
      "loss: 0.2984, acc: 0.8833\n",
      "  Batch   600  of  2,463.    Elapsed: 0:02:57.\n",
      "loss: 0.2610, acc: 0.8875\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:09.\n",
      "loss: 0.3023, acc: 0.8875\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:21.\n",
      "loss: 0.2884, acc: 0.9000\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:33.\n",
      "loss: 0.2972, acc: 0.8792\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:45.\n",
      "loss: 0.3727, acc: 0.8375\n",
      "  Batch   800  of  2,463.    Elapsed: 0:03:56.\n",
      "loss: 0.3239, acc: 0.8958\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:08.\n",
      "loss: 0.3063, acc: 0.8833\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:20.\n",
      "loss: 0.3018, acc: 0.8958\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:32.\n",
      "loss: 0.3866, acc: 0.8542\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:44.\n",
      "loss: 0.2917, acc: 0.8708\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:04:55.\n",
      "loss: 0.4456, acc: 0.8208\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:07.\n",
      "loss: 0.4019, acc: 0.8292\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:19.\n",
      "loss: 0.2489, acc: 0.9125\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:31.\n",
      "loss: 0.3786, acc: 0.8458\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:42.\n",
      "loss: 0.3755, acc: 0.8542\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:05:54.\n",
      "loss: 0.3338, acc: 0.8583\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:06.\n",
      "loss: 0.4050, acc: 0.8417\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:18.\n",
      "loss: 0.3608, acc: 0.8375\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:30.\n",
      "loss: 0.3373, acc: 0.8875\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:41.\n",
      "loss: 0.3809, acc: 0.8458\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:06:53.\n",
      "loss: 0.3265, acc: 0.8583\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:05.\n",
      "loss: 0.2573, acc: 0.9125\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:17.\n",
      "loss: 0.3982, acc: 0.8375\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:29.\n",
      "loss: 0.2019, acc: 0.9417\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:07:40.\n",
      "loss: 0.3475, acc: 0.8667\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:07:52.\n",
      "loss: 0.2731, acc: 0.9083\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:04.\n",
      "loss: 0.3534, acc: 0.8583\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:16.\n",
      "loss: 0.3592, acc: 0.8750\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:27.\n",
      "loss: 0.3636, acc: 0.8500\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:08:39.\n",
      "loss: 0.3056, acc: 0.8875\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:08:51.\n",
      "loss: 0.3102, acc: 0.8750\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:03.\n",
      "loss: 0.3550, acc: 0.8583\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:14.\n",
      "loss: 0.3336, acc: 0.8583\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:26.\n",
      "loss: 0.3517, acc: 0.8708\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:09:38.\n",
      "loss: 0.2909, acc: 0.8667\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:09:50.\n",
      "loss: 0.3039, acc: 0.8750\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:10:02.\n",
      "loss: 0.3333, acc: 0.8625\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:13.\n",
      "loss: 0.2787, acc: 0.8958\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:25.\n",
      "loss: 0.3601, acc: 0.8750\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:10:37.\n",
      "loss: 0.3124, acc: 0.8750\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:10:49.\n",
      "loss: 0.2599, acc: 0.8833\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:11:00.\n",
      "loss: 0.3082, acc: 0.8792\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:12.\n",
      "loss: 0.3641, acc: 0.8583\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:24.\n",
      "loss: 0.3199, acc: 0.8625\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:11:36.\n",
      "loss: 0.3237, acc: 0.8917\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:11:48.\n",
      "loss: 0.3369, acc: 0.8458\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:11:59.\n",
      "loss: 0.3492, acc: 0.8583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      2553\n",
      "           1       0.72      0.72      0.72      1232\n",
      "           2       0.81      0.80      0.81      1358\n",
      "\n",
      "    accuracy                           0.83      5143\n",
      "   macro avg       0.81      0.81      0.81      5143\n",
      "weighted avg       0.83      0.83      0.83      5143\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,463.    Elapsed: 0:00:12.\n",
      "loss: 0.1741, acc: 0.9431\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:24.\n",
      "loss: 0.1533, acc: 0.9375\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:36.\n",
      "loss: 0.1881, acc: 0.9417\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:48.\n",
      "loss: 0.0886, acc: 0.9583\n",
      "  Batch   200  of  2,463.    Elapsed: 0:00:59.\n",
      "loss: 0.1714, acc: 0.9417\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:11.\n",
      "loss: 0.1947, acc: 0.9542\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:23.\n",
      "loss: 0.2115, acc: 0.9167\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:35.\n",
      "loss: 0.1950, acc: 0.9208\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:47.\n",
      "loss: 0.1436, acc: 0.9458\n",
      "  Batch   400  of  2,463.    Elapsed: 0:01:58.\n",
      "loss: 0.1408, acc: 0.9583\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:10.\n",
      "loss: 0.2005, acc: 0.9250\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:22.\n",
      "loss: 0.2078, acc: 0.9417\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:34.\n",
      "loss: 0.1839, acc: 0.9208\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:46.\n",
      "loss: 0.1428, acc: 0.9500\n",
      "  Batch   600  of  2,463.    Elapsed: 0:02:57.\n",
      "loss: 0.1638, acc: 0.9375\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:09.\n",
      "loss: 0.1675, acc: 0.9292\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:21.\n",
      "loss: 0.1927, acc: 0.9333\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:33.\n",
      "loss: 0.2432, acc: 0.9000\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:44.\n",
      "loss: 0.1606, acc: 0.9417\n",
      "  Batch   800  of  2,463.    Elapsed: 0:03:56.\n",
      "loss: 0.1505, acc: 0.9458\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:08.\n",
      "loss: 0.2267, acc: 0.9083\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:20.\n",
      "loss: 0.2482, acc: 0.9125\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:32.\n",
      "loss: 0.2037, acc: 0.9208\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:43.\n",
      "loss: 0.2182, acc: 0.9167\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:04:55.\n",
      "loss: 0.2157, acc: 0.9333\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:07.\n",
      "loss: 0.1648, acc: 0.9500\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:19.\n",
      "loss: 0.1794, acc: 0.9333\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:30.\n",
      "loss: 0.2021, acc: 0.9250\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:42.\n",
      "loss: 0.1826, acc: 0.9375\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:05:54.\n",
      "loss: 0.1978, acc: 0.9250\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:06.\n",
      "loss: 0.2645, acc: 0.8917\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:18.\n",
      "loss: 0.2586, acc: 0.9042\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:29.\n",
      "loss: 0.1715, acc: 0.9292\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:41.\n",
      "loss: 0.2365, acc: 0.9042\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:06:53.\n",
      "loss: 0.1200, acc: 0.9750\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:05.\n",
      "loss: 0.1846, acc: 0.9500\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:17.\n",
      "loss: 0.1926, acc: 0.9292\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "epochs=4\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "#     total_loss=0\n",
    "    model.train()\n",
    "    n_correct, n_total, loss_total = 0, 0, 0\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        a_input_ids  = batch[0].to(device)\n",
    "        a_input_mask = batch[1].to(device)\n",
    "        b_input_ids  = batch[2].to(device)\n",
    "        b_input_mask = batch[3].to(device)\n",
    "        labels       = batch[4].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs=a_input_ids,a_input_mask,b_input_ids,b_input_mask \n",
    "        predict=model(inputs)\n",
    "        loss=criterion(predict,labels)\n",
    "#         print(predict,labels,loss)\n",
    "#         total_loss+=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_correct += (torch.argmax(predict, -1) == labels).sum().item()\n",
    "        n_total += len(predict)\n",
    "        loss_total += loss.item() * len(predict)\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))        \n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "            loss_plot.append(train_loss)\n",
    "            acc_plot.append(train_acc)\n",
    "            logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))   \n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            \n",
    "    true_labels,predict_labels=[],[]        \n",
    "    for batch in validation_dataloader:\n",
    "        a_input_ids  = batch[0].to(device)\n",
    "        a_input_mask = batch[1].to(device)\n",
    "        b_input_ids  = batch[2].to(device)\n",
    "        b_input_mask = batch[3].to(device)\n",
    "        labels       = batch[4]\n",
    "        inputs=a_input_ids,a_input_mask,b_input_ids,b_input_mask\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(inputs)        \n",
    "        predict=outputs.detach().cpu().numpy()\n",
    "        predict=np.argmax(predict, axis=1).flatten()\n",
    "        predict_labels.append(predict)\n",
    "        true_labels.append(labels)\n",
    "    true_labels=[y for x in true_labels for y in x]\n",
    "    predict_labels=[y for x in predict_labels for y in x]\n",
    "    f1_score=f1_score(true_labels,predict_labels,average='weighted')\n",
    "    print(classification_report(true_labels,predict_labels))\n",
    "    print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Training loss and accuracy\") \n",
    "plt.xlabel(\"train epoch\") \n",
    "plt.ylabel(\"acc or loss\") \n",
    "plt.plot(loss_plot, color='firebrick',   label='training loss')\n",
    "plt.plot(acc_plot,  color='darkorange',   label='accuracy')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
