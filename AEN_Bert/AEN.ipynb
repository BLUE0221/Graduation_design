{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "data_process=imp.load_source('data_process','../data_process.py')\n",
    "from data_process import get_xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents,train_contexts,train_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Train.xml')\n",
    "validation_sents,validation_contexts,validation_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Dev.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name='hfl/chinese-bert-wwm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(bert_name, return_dict=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertTensor(text_list,MAX_LEN = 128):\n",
    "    words_idx = []\n",
    "    for sent in text_list:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = MAX_LEN,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        \n",
    "        words_idx.append(encoded_sent)\n",
    "    \n",
    "    words_idx=pad_sequences(words_idx, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    words_masks=[]\n",
    "    for sent in words_idx:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        words_masks.append(att_mask)\n",
    "        \n",
    "    return torch.tensor(words_idx),torch.tensor(words_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "train_sents,train_sents_masks=get_bertTensor(train_sents)\n",
    "train_contexts,train_contexts_masks=get_bertTensor(train_contexts,MAX_LEN=256)\n",
    "train_labels=torch.tensor(train_labels)\n",
    "\n",
    "validation_sents,validation_sents_masks=get_bertTensor(validation_sents)\n",
    "validation_contexts,validation_contexts_masks=get_bertTensor(validation_contexts,MAX_LEN=256)\n",
    "validation_labels=torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "train_data = TensorDataset(train_sents,train_sents_masks,train_contexts,train_contexts_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_sents,validation_sents_masks,validation_contexts,\n",
    "                                validation_contexts_masks,validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 AEN_Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "from layers.squeeze_embedding import SqueezeEmbedding\n",
    "from layers.attention import Attention, NoQueryAttention\n",
    "from layers.point_wise_feed_forward import PositionwiseFeedForward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEN_BERT(nn.Module):\n",
    "    def __init__(self,bert):\n",
    "        super(AEN_BERT, self).__init__()\n",
    "\n",
    "        dropout=0.1\n",
    "        bert_dim=768    \n",
    "        hidden_dim=300\n",
    "        polarities_dim=3\n",
    "        \n",
    "        self.drop_path_prob=0.0\n",
    "        self.bert=bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_k = Attention(bert_dim, out_dim=hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.attn_q = Attention(bert_dim, out_dim=hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.ffn_c = PositionwiseFeedForward(hidden_dim, dropout=dropout)\n",
    "        self.ffn_t = PositionwiseFeedForward(hidden_dim, dropout=dropout)\n",
    "\n",
    "        self.attn_s1 = Attention(hidden_dim, n_head=8, score_function='mlp', dropout=dropout)\n",
    "        self.dense = nn.Linear(hidden_dim*3, polarities_dim)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "#         context, target = inputs[0], inputs[1]\n",
    "        context_len=128\n",
    "        target_len=128\n",
    "        \n",
    "        target,_=bert(a_input_ids, token_type_ids=None, attention_mask=a_input_mask,)\n",
    "        context,_=bert(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,)\n",
    "#         context_len = torch.sum(context != 0, dim=-1)\n",
    "#         target_len = torch.sum(target != 0, dim=-1)\n",
    "#         context = self.squeeze_embedding(context, context_len)\n",
    "#         context, _ = self.bert(context)\n",
    "        context = self.dropout(context)\n",
    "        target = self.dropout(target)\n",
    "\n",
    "        hc, _ = self.attn_k(context, context)\n",
    "        hc = self.ffn_c(hc)\n",
    "        ht, _ = self.attn_q(context, target)\n",
    "        ht = self.ffn_t(ht)\n",
    "\n",
    "        s1, _ = self.attn_s1(hc, ht)\n",
    "\n",
    "        hc_mean = torch.div(torch.sum(hc, dim=1), context_len)\n",
    "        ht_mean = torch.div(torch.sum(ht, dim=1), target_len)\n",
    "        s1_mean = torch.div(torch.sum(s1, dim=1), context_len)\n",
    "\n",
    "        x = torch.cat((hc_mean, s1_mean, ht_mean), dim=-1)\n",
    "        out=self.softmax(self.dense(x))\n",
    "#         out = torch.argmax(self.softmax(self.dense(x)),dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0,2,3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda:0\")#select gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert.cuda()\n",
    "model=AEN_BERT(bert)\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  2,463.    Elapsed: 0:00:13.\n",
      "loss: 1.0116, acc: 0.5366\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:25.\n",
      "loss: 0.9801, acc: 0.5535\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:37.\n",
      "loss: 0.9584, acc: 0.5799\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:49.\n",
      "loss: 0.9382, acc: 0.6004\n",
      "  Batch   200  of  2,463.    Elapsed: 0:01:01.\n",
      "loss: 0.9351, acc: 0.6036\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:13.\n",
      "loss: 0.9322, acc: 0.6051\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:25.\n",
      "loss: 0.9258, acc: 0.6139\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:37.\n",
      "loss: 0.9202, acc: 0.6210\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:49.\n",
      "loss: 0.9147, acc: 0.6265\n",
      "  Batch   400  of  2,463.    Elapsed: 0:02:01.\n",
      "loss: 0.9146, acc: 0.6276\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:14.\n",
      "loss: 0.9099, acc: 0.6327\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:26.\n",
      "loss: 0.9084, acc: 0.6355\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:38.\n",
      "loss: 0.9108, acc: 0.6318\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:50.\n",
      "loss: 0.9119, acc: 0.6307\n",
      "  Batch   600  of  2,463.    Elapsed: 0:03:02.\n",
      "loss: 0.9114, acc: 0.6314\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:15.\n",
      "loss: 0.9089, acc: 0.6347\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:27.\n",
      "loss: 0.9095, acc: 0.6344\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:39.\n",
      "loss: 0.9045, acc: 0.6396\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:51.\n",
      "loss: 0.9030, acc: 0.6410\n",
      "  Batch   800  of  2,463.    Elapsed: 0:04:03.\n",
      "loss: 0.9023, acc: 0.6417\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:16.\n",
      "loss: 0.9001, acc: 0.6439\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:28.\n",
      "loss: 0.8998, acc: 0.6443\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:40.\n",
      "loss: 0.8986, acc: 0.6457\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:52.\n",
      "loss: 0.8962, acc: 0.6483\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:05:04.\n",
      "loss: 0.8939, acc: 0.6507\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:17.\n",
      "loss: 0.8914, acc: 0.6534\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:29.\n",
      "loss: 0.8895, acc: 0.6551\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:41.\n",
      "loss: 0.8898, acc: 0.6549\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:53.\n",
      "loss: 0.8880, acc: 0.6568\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:06:05.\n",
      "loss: 0.8868, acc: 0.6572\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:18.\n",
      "loss: 0.8851, acc: 0.6590\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:30.\n",
      "loss: 0.8841, acc: 0.6603\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:42.\n",
      "loss: 0.8840, acc: 0.6606\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:54.\n",
      "loss: 0.8826, acc: 0.6621\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:07:06.\n",
      "loss: 0.8807, acc: 0.6642\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:18.\n",
      "loss: 0.8803, acc: 0.6644\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:31.\n",
      "loss: 0.8775, acc: 0.6673\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:43.\n",
      "loss: 0.8774, acc: 0.6673\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:07:55.\n",
      "loss: 0.8769, acc: 0.6677\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:08:07.\n",
      "loss: 0.8756, acc: 0.6692\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:19.\n",
      "loss: 0.8771, acc: 0.6675\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:31.\n",
      "loss: 0.8761, acc: 0.6686\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:44.\n",
      "loss: 0.8741, acc: 0.6707\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:08:56.\n",
      "loss: 0.8722, acc: 0.6728\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:09:08.\n",
      "loss: 0.8717, acc: 0.6731\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:20.\n",
      "loss: 0.8702, acc: 0.6749\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:32.\n",
      "loss: 0.8694, acc: 0.6760\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:44.\n",
      "loss: 0.8688, acc: 0.6768\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:09:57.\n",
      "loss: 0.8693, acc: 0.6761\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:10:09.\n",
      "loss: 0.8687, acc: 0.6767\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:10:21.\n",
      "loss: 0.8680, acc: 0.6775\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:33.\n",
      "loss: 0.8685, acc: 0.6770\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:45.\n",
      "loss: 0.8681, acc: 0.6774\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:10:58.\n",
      "loss: 0.8673, acc: 0.6781\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:11:10.\n",
      "loss: 0.8670, acc: 0.6779\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:11:22.\n",
      "loss: 0.8667, acc: 0.6782\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:34.\n",
      "loss: 0.8658, acc: 0.6787\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:46.\n",
      "loss: 0.8650, acc: 0.6796\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:11:59.\n",
      "loss: 0.8644, acc: 0.6802\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:12:11.\n",
      "loss: 0.8632, acc: 0.6816\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:12:23.\n",
      "loss: 0.8629, acc: 0.6820\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,463.    Elapsed: 0:00:15.\n",
      "loss: 0.8255, acc: 0.7276\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:27.\n",
      "loss: 0.8161, acc: 0.7305\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:39.\n",
      "loss: 0.8102, acc: 0.7369\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:52.\n",
      "loss: 0.8199, acc: 0.7267\n",
      "  Batch   200  of  2,463.    Elapsed: 0:01:04.\n",
      "loss: 0.8231, acc: 0.7239\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:16.\n",
      "loss: 0.8111, acc: 0.7365\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:28.\n",
      "loss: 0.8098, acc: 0.7355\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:40.\n",
      "loss: 0.8100, acc: 0.7357\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:53.\n",
      "loss: 0.8134, acc: 0.7327\n",
      "  Batch   400  of  2,463.    Elapsed: 0:02:05.\n",
      "loss: 0.8127, acc: 0.7344\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:17.\n",
      "loss: 0.8067, acc: 0.7407\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:29.\n",
      "loss: 0.8084, acc: 0.7391\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:41.\n",
      "loss: 0.8059, acc: 0.7412\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:54.\n",
      "loss: 0.8046, acc: 0.7424\n",
      "  Batch   600  of  2,463.    Elapsed: 0:03:06.\n",
      "loss: 0.8078, acc: 0.7393\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:18.\n",
      "loss: 0.8057, acc: 0.7421\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:30.\n",
      "loss: 0.8105, acc: 0.7374\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:43.\n",
      "loss: 0.8172, acc: 0.7307\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:55.\n",
      "loss: 0.8183, acc: 0.7293\n",
      "  Batch   800  of  2,463.    Elapsed: 0:04:07.\n",
      "loss: 0.8203, acc: 0.7272\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:19.\n",
      "loss: 0.8228, acc: 0.7243\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:32.\n",
      "loss: 0.8219, acc: 0.7253\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:44.\n",
      "loss: 0.8212, acc: 0.7257\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:56.\n",
      "loss: 0.8218, acc: 0.7251\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:05:08.\n",
      "loss: 0.8244, acc: 0.7226\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:20.\n",
      "loss: 0.8254, acc: 0.7217\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:33.\n",
      "loss: 0.8244, acc: 0.7229\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:45.\n",
      "loss: 0.8271, acc: 0.7202\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:57.\n",
      "loss: 0.8279, acc: 0.7194\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:06:09.\n",
      "loss: 0.8289, acc: 0.7186\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:21.\n",
      "loss: 0.8280, acc: 0.7197\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:34.\n",
      "loss: 0.8300, acc: 0.7175\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:46.\n",
      "loss: 0.8300, acc: 0.7173\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:58.\n",
      "loss: 0.8300, acc: 0.7170\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:07:10.\n",
      "loss: 0.8309, acc: 0.7156\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:22.\n",
      "loss: 0.8309, acc: 0.7148\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:35.\n",
      "loss: 0.8308, acc: 0.7149\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:47.\n",
      "loss: 0.8306, acc: 0.7151\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:07:59.\n",
      "loss: 0.8302, acc: 0.7156\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:08:11.\n",
      "loss: 0.8303, acc: 0.7156\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:24.\n",
      "loss: 0.8292, acc: 0.7166\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:36.\n",
      "loss: 0.8283, acc: 0.7173\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:48.\n",
      "loss: 0.8275, acc: 0.7181\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:09:00.\n",
      "loss: 0.8272, acc: 0.7182\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:09:12.\n",
      "loss: 0.8270, acc: 0.7184\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:25.\n",
      "loss: 0.8264, acc: 0.7192\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:37.\n",
      "loss: 0.8260, acc: 0.7197\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:49.\n",
      "loss: 0.8254, acc: 0.7206\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:10:01.\n",
      "loss: 0.8260, acc: 0.7201\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:10:14.\n",
      "loss: 0.8252, acc: 0.7211\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:10:26.\n",
      "loss: 0.8246, acc: 0.7217\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:38.\n",
      "loss: 0.8244, acc: 0.7218\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:50.\n",
      "loss: 0.8231, acc: 0.7232\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:11:02.\n",
      "loss: 0.8220, acc: 0.7244\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:11:15.\n",
      "loss: 0.8221, acc: 0.7244\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:11:27.\n",
      "loss: 0.8221, acc: 0.7244\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:39.\n",
      "loss: 0.8216, acc: 0.7248\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:51.\n",
      "loss: 0.8208, acc: 0.7256\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:12:04.\n",
      "loss: 0.8203, acc: 0.7261\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:12:16.\n",
      "loss: 0.8200, acc: 0.7264\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:12:28.\n",
      "loss: 0.8202, acc: 0.7263\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,463.    Elapsed: 0:00:15.\n",
      "loss: 0.8328, acc: 0.7114\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:27.\n",
      "loss: 0.8198, acc: 0.7222\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:39.\n",
      "loss: 0.8142, acc: 0.7314\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:52.\n",
      "loss: 0.8106, acc: 0.7340\n",
      "  Batch   200  of  2,463.    Elapsed: 0:01:04.\n",
      "loss: 0.8140, acc: 0.7305\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:16.\n",
      "loss: 0.8232, acc: 0.7199\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:28.\n",
      "loss: 0.8257, acc: 0.7171\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:40.\n",
      "loss: 0.8187, acc: 0.7248\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:52.\n",
      "loss: 0.8133, acc: 0.7304\n",
      "  Batch   400  of  2,463.    Elapsed: 0:02:05.\n",
      "loss: 0.8123, acc: 0.7315\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:17.\n",
      "loss: 0.8144, acc: 0.7302\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:29.\n",
      "loss: 0.8150, acc: 0.7297\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:41.\n",
      "loss: 0.8114, acc: 0.7332\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:54.\n",
      "loss: 0.8099, acc: 0.7344\n",
      "  Batch   600  of  2,463.    Elapsed: 0:03:06.\n",
      "loss: 0.8099, acc: 0.7335\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:18.\n",
      "loss: 0.8081, acc: 0.7356\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:30.\n",
      "loss: 0.8081, acc: 0.7354\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:42.\n",
      "loss: 0.8071, acc: 0.7367\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:55.\n",
      "loss: 0.8062, acc: 0.7383\n",
      "  Batch   800  of  2,463.    Elapsed: 0:04:07.\n",
      "loss: 0.8065, acc: 0.7380\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:19.\n",
      "loss: 0.8075, acc: 0.7368\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:31.\n",
      "loss: 0.8085, acc: 0.7363\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:44.\n",
      "loss: 0.8065, acc: 0.7383\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:56.\n",
      "loss: 0.8052, acc: 0.7399\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:05:08.\n",
      "loss: 0.8034, acc: 0.7421\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:20.\n",
      "loss: 0.8023, acc: 0.7434\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:32.\n",
      "loss: 0.8034, acc: 0.7419\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:45.\n",
      "loss: 0.8047, acc: 0.7406\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:57.\n",
      "loss: 0.8049, acc: 0.7405\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:06:09.\n",
      "loss: 0.8052, acc: 0.7402\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:21.\n",
      "loss: 0.8055, acc: 0.7397\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:33.\n",
      "loss: 0.8050, acc: 0.7404\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:46.\n",
      "loss: 0.8054, acc: 0.7398\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:58.\n",
      "loss: 0.8048, acc: 0.7400\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:07:10.\n",
      "loss: 0.8046, acc: 0.7403\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:22.\n",
      "loss: 0.8043, acc: 0.7407\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:35.\n",
      "loss: 0.8039, acc: 0.7412\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:47.\n",
      "loss: 0.8043, acc: 0.7410\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:07:59.\n",
      "loss: 0.8022, acc: 0.7431\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:08:11.\n",
      "loss: 0.8028, acc: 0.7426\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:23.\n",
      "loss: 0.8032, acc: 0.7423\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:35.\n",
      "loss: 0.8026, acc: 0.7429\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:48.\n",
      "loss: 0.8031, acc: 0.7426\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:09:00.\n",
      "loss: 0.8031, acc: 0.7425\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:09:12.\n",
      "loss: 0.8023, acc: 0.7432\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:24.\n",
      "loss: 0.8023, acc: 0.7433\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:36.\n",
      "loss: 0.8024, acc: 0.7433\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:49.\n",
      "loss: 0.8017, acc: 0.7441\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:10:01.\n",
      "loss: 0.8020, acc: 0.7438\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:10:13.\n",
      "loss: 0.8019, acc: 0.7440\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:10:25.\n",
      "loss: 0.8014, acc: 0.7446\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:37.\n",
      "loss: 0.8011, acc: 0.7449\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:49.\n",
      "loss: 0.8016, acc: 0.7444\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:11:02.\n",
      "loss: 0.8015, acc: 0.7446\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:11:14.\n",
      "loss: 0.8012, acc: 0.7448\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:11:26.\n",
      "loss: 0.8014, acc: 0.7446\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:38.\n",
      "loss: 0.8010, acc: 0.7448\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:50.\n",
      "loss: 0.8021, acc: 0.7437\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:12:02.\n",
      "loss: 0.8019, acc: 0.7440\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:12:15.\n",
      "loss: 0.8015, acc: 0.7446\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:12:27.\n",
      "loss: 0.8012, acc: 0.7449\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  2,463.    Elapsed: 0:00:15.\n",
      "loss: 0.7592, acc: 0.7927\n",
      "  Batch    80  of  2,463.    Elapsed: 0:00:27.\n",
      "loss: 0.7947, acc: 0.7551\n",
      "  Batch   120  of  2,463.    Elapsed: 0:00:39.\n",
      "loss: 0.8100, acc: 0.7410\n",
      "  Batch   160  of  2,463.    Elapsed: 0:00:52.\n",
      "loss: 0.8310, acc: 0.7195\n",
      "  Batch   200  of  2,463.    Elapsed: 0:01:04.\n",
      "loss: 0.8312, acc: 0.7164\n",
      "  Batch   240  of  2,463.    Elapsed: 0:01:16.\n",
      "loss: 0.8230, acc: 0.7248\n",
      "  Batch   280  of  2,463.    Elapsed: 0:01:28.\n",
      "loss: 0.8202, acc: 0.7272\n",
      "  Batch   320  of  2,463.    Elapsed: 0:01:40.\n",
      "loss: 0.8209, acc: 0.7259\n",
      "  Batch   360  of  2,463.    Elapsed: 0:01:52.\n",
      "loss: 0.8242, acc: 0.7230\n",
      "  Batch   400  of  2,463.    Elapsed: 0:02:05.\n",
      "loss: 0.8248, acc: 0.7224\n",
      "  Batch   440  of  2,463.    Elapsed: 0:02:17.\n",
      "loss: 0.8207, acc: 0.7271\n",
      "  Batch   480  of  2,463.    Elapsed: 0:02:29.\n",
      "loss: 0.8182, acc: 0.7297\n",
      "  Batch   520  of  2,463.    Elapsed: 0:02:41.\n",
      "loss: 0.8134, acc: 0.7348\n",
      "  Batch   560  of  2,463.    Elapsed: 0:02:53.\n",
      "loss: 0.8146, acc: 0.7332\n",
      "  Batch   600  of  2,463.    Elapsed: 0:03:05.\n",
      "loss: 0.8127, acc: 0.7332\n",
      "  Batch   640  of  2,463.    Elapsed: 0:03:18.\n",
      "loss: 0.8130, acc: 0.7322\n",
      "  Batch   680  of  2,463.    Elapsed: 0:03:30.\n",
      "loss: 0.8101, acc: 0.7354\n",
      "  Batch   720  of  2,463.    Elapsed: 0:03:42.\n",
      "loss: 0.8092, acc: 0.7365\n",
      "  Batch   760  of  2,463.    Elapsed: 0:03:54.\n",
      "loss: 0.8077, acc: 0.7385\n",
      "  Batch   800  of  2,463.    Elapsed: 0:04:06.\n",
      "loss: 0.8095, acc: 0.7368\n",
      "  Batch   840  of  2,463.    Elapsed: 0:04:18.\n",
      "loss: 0.8087, acc: 0.7374\n",
      "  Batch   880  of  2,463.    Elapsed: 0:04:31.\n",
      "loss: 0.8099, acc: 0.7359\n",
      "  Batch   920  of  2,463.    Elapsed: 0:04:43.\n",
      "loss: 0.8110, acc: 0.7345\n",
      "  Batch   960  of  2,463.    Elapsed: 0:04:55.\n",
      "loss: 0.8126, acc: 0.7324\n",
      "  Batch 1,000  of  2,463.    Elapsed: 0:05:07.\n",
      "loss: 0.8127, acc: 0.7321\n",
      "  Batch 1,040  of  2,463.    Elapsed: 0:05:19.\n",
      "loss: 0.8142, acc: 0.7310\n",
      "  Batch 1,080  of  2,463.    Elapsed: 0:05:32.\n",
      "loss: 0.8162, acc: 0.7293\n",
      "  Batch 1,120  of  2,463.    Elapsed: 0:05:44.\n",
      "loss: 0.8176, acc: 0.7279\n",
      "  Batch 1,160  of  2,463.    Elapsed: 0:05:56.\n",
      "loss: 0.8199, acc: 0.7249\n",
      "  Batch 1,200  of  2,463.    Elapsed: 0:06:08.\n",
      "loss: 0.8212, acc: 0.7234\n",
      "  Batch 1,240  of  2,463.    Elapsed: 0:06:21.\n",
      "loss: 0.8209, acc: 0.7243\n",
      "  Batch 1,280  of  2,463.    Elapsed: 0:06:33.\n",
      "loss: 0.8207, acc: 0.7244\n",
      "  Batch 1,320  of  2,463.    Elapsed: 0:06:45.\n",
      "loss: 0.8194, acc: 0.7258\n",
      "  Batch 1,360  of  2,463.    Elapsed: 0:06:57.\n",
      "loss: 0.8193, acc: 0.7258\n",
      "  Batch 1,400  of  2,463.    Elapsed: 0:07:09.\n",
      "loss: 0.8174, acc: 0.7281\n",
      "  Batch 1,440  of  2,463.    Elapsed: 0:07:22.\n",
      "loss: 0.8166, acc: 0.7289\n",
      "  Batch 1,480  of  2,463.    Elapsed: 0:07:34.\n",
      "loss: 0.8163, acc: 0.7293\n",
      "  Batch 1,520  of  2,463.    Elapsed: 0:07:46.\n",
      "loss: 0.8150, acc: 0.7308\n",
      "  Batch 1,560  of  2,463.    Elapsed: 0:07:58.\n",
      "loss: 0.8150, acc: 0.7308\n",
      "  Batch 1,600  of  2,463.    Elapsed: 0:08:10.\n",
      "loss: 0.8139, acc: 0.7321\n",
      "  Batch 1,640  of  2,463.    Elapsed: 0:08:23.\n",
      "loss: 0.8139, acc: 0.7322\n",
      "  Batch 1,680  of  2,463.    Elapsed: 0:08:35.\n",
      "loss: 0.8130, acc: 0.7330\n",
      "  Batch 1,720  of  2,463.    Elapsed: 0:08:47.\n",
      "loss: 0.8121, acc: 0.7340\n",
      "  Batch 1,760  of  2,463.    Elapsed: 0:08:59.\n",
      "loss: 0.8140, acc: 0.7322\n",
      "  Batch 1,800  of  2,463.    Elapsed: 0:09:12.\n",
      "loss: 0.8185, acc: 0.7273\n",
      "  Batch 1,840  of  2,463.    Elapsed: 0:09:24.\n",
      "loss: 0.8249, acc: 0.7204\n",
      "  Batch 1,880  of  2,463.    Elapsed: 0:09:36.\n",
      "loss: 0.8259, acc: 0.7192\n",
      "  Batch 1,920  of  2,463.    Elapsed: 0:09:48.\n",
      "loss: 0.8262, acc: 0.7191\n",
      "  Batch 1,960  of  2,463.    Elapsed: 0:10:00.\n",
      "loss: 0.8261, acc: 0.7193\n",
      "  Batch 2,000  of  2,463.    Elapsed: 0:10:13.\n",
      "loss: 0.8251, acc: 0.7205\n",
      "  Batch 2,040  of  2,463.    Elapsed: 0:10:25.\n",
      "loss: 0.8244, acc: 0.7213\n",
      "  Batch 2,080  of  2,463.    Elapsed: 0:10:37.\n",
      "loss: 0.8231, acc: 0.7226\n",
      "  Batch 2,120  of  2,463.    Elapsed: 0:10:49.\n",
      "loss: 0.8236, acc: 0.7223\n",
      "  Batch 2,160  of  2,463.    Elapsed: 0:11:01.\n",
      "loss: 0.8225, acc: 0.7234\n",
      "  Batch 2,200  of  2,463.    Elapsed: 0:11:14.\n",
      "loss: 0.8231, acc: 0.7226\n",
      "  Batch 2,240  of  2,463.    Elapsed: 0:11:26.\n",
      "loss: 0.8228, acc: 0.7227\n",
      "  Batch 2,280  of  2,463.    Elapsed: 0:11:38.\n",
      "loss: 0.8231, acc: 0.7223\n",
      "  Batch 2,320  of  2,463.    Elapsed: 0:11:50.\n",
      "loss: 0.8236, acc: 0.7217\n",
      "  Batch 2,360  of  2,463.    Elapsed: 0:12:03.\n",
      "loss: 0.8241, acc: 0.7212\n",
      "  Batch 2,400  of  2,463.    Elapsed: 0:12:15.\n",
      "loss: 0.8240, acc: 0.7212\n",
      "  Batch 2,440  of  2,463.    Elapsed: 0:12:27.\n",
      "loss: 0.8236, acc: 0.7214\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "epochs=4\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    total_loss=0\n",
    "    model.train()\n",
    "    n_correct, n_total, loss_total = 0, 0, 0\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        a_input_ids  = batch[0].to(device)\n",
    "        a_input_mask = batch[1].to(device)\n",
    "        b_input_ids  = batch[2].to(device)\n",
    "        b_input_mask = batch[3].to(device)\n",
    "        labels       = batch[4].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs=a_input_ids,a_input_mask,b_input_ids,b_input_mask \n",
    "        predict=model(inputs)\n",
    "        loss=criterion(predict,labels)\n",
    "        \n",
    "        total_loss+=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_correct += (torch.argmax(predict, -1) == labels).sum().item()\n",
    "        n_total += len(predict)\n",
    "        loss_total += loss.item() * len(predict)\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))        \n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "            logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))   \n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
