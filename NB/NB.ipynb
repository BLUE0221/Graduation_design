{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "data_process=imp.load_source('data_process','../data_process.py')\n",
    "from data_process import get_xml_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text,_,y=get_xml_data('../SMP2019/SMP2019_ECISA_Train.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data[[\"text\"]]\n",
    "# y = data[[\"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpsoxf05vl' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.840 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "def chinese_word_cut(mytext):\n",
    "    return [\" \".join(jieba.cut(temp)) for temp in mytext]\n",
    "\n",
    "cuted_text = chinese_word_cut(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuted_text=np.array(cuted_text).reshape(len(cuted_text),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cuted_text, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_train, y_train= rus.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['老舍 笔下 的 “ 九曲十八弯 ” 属 中 俄 界河 额尔古纳河 水系 。'],\n",
       "       ['到 售票口 买 完 门票 后 、 还 需要 坐 景区 大巴 前往 ！'],\n",
       "       ['\\\\ \\\\ @ SarahMis   : 真的 是 很 卧槽 ！ ！'],\n",
       "       ...,\n",
       "       ['现在 没有 了 ， 已经 做过 三次 手术 ， 我 感觉 要犯 了'],\n",
       "       ['老 觉得 无 烟味 ， 但 有 电池 味 … 应该 是 使用不当 …'],\n",
       "       ['一个多月 前 ， 这个 自称 “ 不看 电影 ， 也 不 看电视 ， 只 看 英文 小说 ” 的 房地产 大亨 ， 出席 了 乐视 影业 姗姗来迟 的 年度 发布会 。']],\n",
       "      dtype='<U314')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['老舍 笔下 的 “ 九曲十八弯 ” 属 中 俄 界河 额尔古纳河 水系 。'], dtype='<U314')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopword(stop_word_file):\n",
    "    with open(stop_word_file,'rb') as f:\n",
    "        stop_word = f.read()\n",
    "        \n",
    "    stop_word_list = stop_word.decode().split(\"/n\")\n",
    "    custom_stopword = [i for i in stop_word_list]\n",
    "    return custom_stopword\n",
    "\n",
    "stopwords = get_custom_stopword(\"哈工大停词表.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "# term_matrix = DataFrame(vect.fit_transform(X_train.cuted_comment).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix = DataFrame(vect.fit_transform(X_train.ravel()).toarray(), columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8658, 23125)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "# min_df = 0.00005 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "min_df = 0.001\n",
    "\n",
    "vect = CountVectorizer(max_df = max_df,\n",
    "                       min_df = min_df,\n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "term_matrix = DataFrame(vect.fit_transform(X_train.ravel()).toarray(), columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8658, 1166)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_df=0.8, min_df=0.001,\n",
       "                                 stop_words=frozenset({'———\\r\\n'\n",
       "                                                       '》），\\r\\n'\n",
       "                                                       '）÷（１－\\r\\n'\n",
       "                                                       '”，\\r\\n'\n",
       "                                                       '）、\\r\\n'\n",
       "                                                       '＝（\\r\\n'\n",
       "                                                       ':\\r\\n'\n",
       "                                                       '→\\r\\n'\n",
       "                                                       '℃ \\r\\n'\n",
       "                                                       '&\\r\\n'\n",
       "                                                       '*\\r\\n'\n",
       "                                                       '一一\\r\\n'\n",
       "                                                       '~~~~\\r\\n'\n",
       "                                                       '’\\r\\n'\n",
       "                                                       '. \\r\\n'\n",
       "                                                       '『\\r\\n'\n",
       "                                                       '.一\\r\\n'\n",
       "                                                       './\\r\\n'\n",
       "                                                       '-- \\r\\n'\n",
       "                                                       '』\\r\\n'\n",
       "                                                       '＝″\\r\\n'\n",
       "                                                       '【\\r\\n'\n",
       "                                                       '［＊］\\r\\n'\n",
       "                                                       '｝＞\\r\\n'\n",
       "                                                       '［⑤］］\\r\\n'\n",
       "                                                       '［①Ｄ］\\r\\n'\n",
       "                                                       'ｃ］\\r\\n'\n",
       "                                                       'ｎｇ昉\\r\\n'\n",
       "                                                       '＊\\r\\n'\n",
       "                                                       '//\\r\\n'\n",
       "                                                       '［\\r\\n'\n",
       "                                                       '］\\r...\n",
       "                                                       '至\\r\\n'\n",
       "                                                       '至于\\r\\n'\n",
       "                                                       '诸位\\r\\n'\n",
       "                                                       '着\\r\\n'\n",
       "                                                       '着呢\\r\\n'\n",
       "                                                       '自\\r\\n'\n",
       "                                                       '自从\\r\\n'\n",
       "                                                       '自个儿\\r\\n'\n",
       "                                                       '自各儿\\r\\n'\n",
       "                                                       '自己\\r\\n'\n",
       "                                                       '自家\\r\\n'\n",
       "                                                       '自身\\r\\n'\n",
       "                                                       '综上所述\\r\\n'\n",
       "                                                       '总的来看\\r\\n'\n",
       "                                                       '总的来说\\r\\n'\n",
       "                                                       '总的说来\\r\\n'\n",
       "                                                       '总而言之\\r\\n'\n",
       "                                                       '总之\\r\\n'\n",
       "                                                       '纵\\r\\n'\n",
       "                                                       '纵令\\r\\n'\n",
       "                                                       '纵然\\r\\n'\n",
       "                                                       '纵使\\r\\n'\n",
       "                                                       '遵照\\r\\n'\n",
       "                                                       '作为\\r\\n'\n",
       "                                                       '兮\\r\\n'\n",
       "                                                       '呃\\r\\n'\n",
       "                                                       '呗\\r\\n'\n",
       "                                                       '咚\\r\\n'\n",
       "                                                       '咦\\r\\n'\n",
       "                                                       '喏\\r\\n'\n",
       "                                                       '啐\\r\\n'\n",
       "                                                       '喔唷\\r\\n'\n",
       "                                                       '嗬\\r\\n'\n",
       "                                                       '嗯\\r\\n'\n",
       "                                                       '嗳'}),\n",
       "                                 token_pattern='(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b')),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(vect, nb)\n",
    "pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', 'sub', 'sup', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｃ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '一一', '一个', '一些', '一切', '一则', '一方面', '一旦', '一来', '一样', '一番', '一直', '一般', '万一', '上下', '不仅', '不但', '不光', '不单', '不只', '不如', '不怕', '不惟', '不成', '不拘', '不比', '不然', '不特', '不独', '不管', '不论', '不过', '不问', '与其', '与否', '与此同时', '两者', '为了', '为什么', '为何', '为着', '乃至', '之一', '之所以', '之类', '乌乎', '也好', '也就是说', '也罢', '于是', '于是乎', '云云', '人家', '什么', '什么样', '从而', '他人', '他们', '以便', '以免', '以及', '以至', '以至于', '以致', '任何', '任凭', '似的', '但是', '何况', '何处', '何时', '作为', '你们', '使得', '例如', '依照', '俺们', '倘使', '倘或', '倘然', '倘若', '假使', '假如', '假若', '关于', '其一', '其中', '其二', '其他', '其余', '其它', '其次', '具体地说', '具体说来', '再者', '再说', '况且', '几时', '凭借', '别的', '别说', '前后', '前者', '加之', '即令', '即使', '即便', '即或', '即若', '及其', '及至', '反之', '反过来', '反过来说', '另一方面', '另外', '只是', '只有', '只要', '只限', '叮咚', '可以', '可是', '可见', '各个', '各位', '各种', '各自', '同时', '向着', '否则', '吧哒', '呜呼', '呼哧', '咱们', '哈哈', '哎呀', '哎哟', '哪个', '哪些', '哪儿', '哪天', '哪年', '哪怕', '哪样', '哪边', '哪里', '哼唷', '啪达', '喔唷', '嗡嗡', '嘎登', '因为', '因此', '因而', '固然', '在下', '多少', '她们', '如上所述', '如何', '如其', '如果', '如此', '如若', '宁可', '宁愿', '宁肯', '它们', '对于', '尔后', '尚且', '就是', '就是说', '尽管', '岂但', '并且', '开外', '开始', '当着', '彼此', '怎么', '怎么办', '怎么样', '怎样', '总之', '总的来看', '总的来说', '总的说来', '总而言之', '恰恰相反', '慢说', '我们', '或是', '或者', '所以', '抑或', '按照', '换句话说', '换言之', '接着', '故此', '旁人', '无宁', '无论', '既是', '既然', '时候', '是的', '有些', '有关', '有的', '有的是', '朝着', '本着', '来着', '极了', '果然', '果真', '某个', '某些', '根据', '正如', '此外', '此间', '毋宁', '每当', '比如', '比方', '沿着', '漫说', '然则', '然后', '然而', '照着', '甚么', '甚而', '甚至', '由于', '由此可见', '的话', '相对而言', '省得', '着呢', '第二', '等等', '紧接着', '纵令', '纵使', '纵然', '经过', '结果', '继而', '综上所述', '罢了', '而且', '而况', '而外', '而已', '而是', '而言', '自个儿', '自从', '自各儿', '自家', '自己', '自身', '至于', '若是', '若非', '莫若', '虽则', '虽然', '虽说', '要不', '要不是', '要不然', '要么', '要是', '许多', '设使', '设若', '诸位', '谁知', '起见', '趁着', '越是', '较之', '还是', '还有', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', '这就是说', '这时', '这样', '这边', '这里', '进而', '连同', '通过', '遵照', '那个', '那么', '那么些', '那么样', '那些', '那会儿', '那儿', '那时', '那样', '那边', '那里', '鄙人', '鉴于', '除了', '除此之外', '除非', '随着', '非但', '非徒', '顺着', '首先', 'ｌｉ', 'ｎｇ昉', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 296, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1199, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 1110, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/home/xuke/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "AttributeError: 'numpy.ndarray' object has no attribute 'lower'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_df=0.8, min_df=0.001,\n",
       "                                 stop_words=frozenset({'———\\r\\n'\n",
       "                                                       '》），\\r\\n'\n",
       "                                                       '）÷（１－\\r\\n'\n",
       "                                                       '”，\\r\\n'\n",
       "                                                       '）、\\r\\n'\n",
       "                                                       '＝（\\r\\n'\n",
       "                                                       ':\\r\\n'\n",
       "                                                       '→\\r\\n'\n",
       "                                                       '℃ \\r\\n'\n",
       "                                                       '&\\r\\n'\n",
       "                                                       '*\\r\\n'\n",
       "                                                       '一一\\r\\n'\n",
       "                                                       '~~~~\\r\\n'\n",
       "                                                       '’\\r\\n'\n",
       "                                                       '. \\r\\n'\n",
       "                                                       '『\\r\\n'\n",
       "                                                       '.一\\r\\n'\n",
       "                                                       './\\r\\n'\n",
       "                                                       '-- \\r\\n'\n",
       "                                                       '』\\r\\n'\n",
       "                                                       '＝″\\r\\n'\n",
       "                                                       '【\\r\\n'\n",
       "                                                       '［＊］\\r\\n'\n",
       "                                                       '｝＞\\r\\n'\n",
       "                                                       '［⑤］］\\r\\n'\n",
       "                                                       '［①Ｄ］\\r\\n'\n",
       "                                                       'ｃ］\\r\\n'\n",
       "                                                       'ｎｇ昉\\r\\n'\n",
       "                                                       '＊\\r\\n'\n",
       "                                                       '//\\r\\n'\n",
       "                                                       '［\\r\\n'\n",
       "                                                       '］\\r...\n",
       "                                                       '至\\r\\n'\n",
       "                                                       '至于\\r\\n'\n",
       "                                                       '诸位\\r\\n'\n",
       "                                                       '着\\r\\n'\n",
       "                                                       '着呢\\r\\n'\n",
       "                                                       '自\\r\\n'\n",
       "                                                       '自从\\r\\n'\n",
       "                                                       '自个儿\\r\\n'\n",
       "                                                       '自各儿\\r\\n'\n",
       "                                                       '自己\\r\\n'\n",
       "                                                       '自家\\r\\n'\n",
       "                                                       '自身\\r\\n'\n",
       "                                                       '综上所述\\r\\n'\n",
       "                                                       '总的来看\\r\\n'\n",
       "                                                       '总的来说\\r\\n'\n",
       "                                                       '总的说来\\r\\n'\n",
       "                                                       '总而言之\\r\\n'\n",
       "                                                       '总之\\r\\n'\n",
       "                                                       '纵\\r\\n'\n",
       "                                                       '纵令\\r\\n'\n",
       "                                                       '纵然\\r\\n'\n",
       "                                                       '纵使\\r\\n'\n",
       "                                                       '遵照\\r\\n'\n",
       "                                                       '作为\\r\\n'\n",
       "                                                       '兮\\r\\n'\n",
       "                                                       '呃\\r\\n'\n",
       "                                                       '呗\\r\\n'\n",
       "                                                       '咚\\r\\n'\n",
       "                                                       '咦\\r\\n'\n",
       "                                                       '喏\\r\\n'\n",
       "                                                       '啐\\r\\n'\n",
       "                                                       '喔唷\\r\\n'\n",
       "                                                       '嗬\\r\\n'\n",
       "                                                       '嗯\\r\\n'\n",
       "                                                       '嗳'}),\n",
       "                                 token_pattern='(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b')),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe.fit(X_train.cuted_comment, y_train)\n",
    "pipe.fit(X_train.ravel(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9cfd8ad6af9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# p=precision_score(y_test, y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# r=recall_score(y_test, y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1176\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1434\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1264\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1266\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "ACU=accuracy_score(y_test, y_pred)\n",
    "# p=precision_score(y_test, y_pred)\n",
    "# r=recall_score(y_test, y_pred)\n",
    "# f1=f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-580d99ae4319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The accruacy score is %.3f   f1_score: %.3f precision_score: %.3f recall_score: %.3f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mACU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f1' is not defined"
     ]
    }
   ],
   "source": [
    "# print(\"The accruacy score is %.3f   f1_score: %.3f precision_score: %.3f recall_score: %.3f \" % (ACU,f1,p,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACU=accuracy_score(y_test, y_pred)\n",
    "# p=precision_score(y_test, y_pred,pos_label=0)\n",
    "# r=recall_score(y_test, y_pred,pos_label=0)\n",
    "# f1=f1_score(y_test, y_pred,pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.654574986464537"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accruacy score is %.3f   f1_score: %.3f precision_score: %.3f recall_score: %.3f \" % (ACU,f1,p,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1265,  286,  165],\n",
       "       [ 220,  520,  202],\n",
       "       [ 168,  235,  633]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
