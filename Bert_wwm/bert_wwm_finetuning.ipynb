{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [BERT Fine-Tuning Tutorial with PyTorch for Text Classification on The Corpus of Linguistic Acceptability (COLA) Dataset.](https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")#select gpu\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Loading SMP2019 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "data_process=imp.load_source('data_process','../data_process.py')\n",
    "from data_process import get_xml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents,_,train_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Train.xml')\n",
    "validation_sents,_,validation_labels=get_xml_data('../SMP2019/SMP2019_ECISA_Dev.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenization & Input Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=['hfl/chinese-bert-wwm','hfl/chinese-bert-wwm-ext','hfl/chinese-roberta-wwm-ext','hfl/chinese-roberta-wwm-ext-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='hfl/chinese-bert-wwm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xuke/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Required Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  别以为政治与你无关，有人送我蒙牛的产品我会以为对方要害我！\n",
      "Token IDs: [101, 1166, 809, 711, 3124, 3780, 680, 872, 3187, 1068, 8024, 3300, 782, 6843, 2769, 5885, 4281, 4638, 772, 1501, 2769, 833, 809, 711, 2190, 3175, 6206, 2154, 2769, 8013, 102]\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "for sent in train_sents:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    train_inputs.append(encoded_sent)\n",
    "print('Original: ', train_sents[0])\n",
    "print('Token IDs:', train_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  曼听公园以前是傣王御花园，有1300多年的历史。\n",
      "Token IDs: [101, 3294, 1420, 1062, 1736, 809, 1184, 3221, 994, 4374, 2539, 5709, 1736, 8024, 3300, 8925, 1914, 2399, 4638, 1325, 1380, 511, 102]\n"
     ]
    }
   ],
   "source": [
    "validation_inputs = []\n",
    "for sent in validation_sents:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    validation_inputs.append(encoded_sent)\n",
    "print('Original: ',validation_sents[0])\n",
    "print('Token IDs:', validation_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Padding & Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  128\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in train_inputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  128\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in validation_inputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 128 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\\Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 128\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "train_inputs = pad_sequences(train_inputs, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "validation_inputs = pad_sequences(validation_inputs, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "print('\\Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  别以为政治与你无关，有人送我蒙牛的产品我会以为对方要害我！\n",
      "Token IDs: [ 101 1166  809  711 3124 3780  680  872 3187 1068 8024 3300  782 6843\n",
      " 2769 5885 4281 4638  772 1501 2769  833  809  711 2190 3175 6206 2154\n",
      " 2769 8013  102    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print('Original: ', train_sents[0])\n",
    "print('Token IDs:', train_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "train_masks = []\n",
    "for sent in train_inputs:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    train_masks.append(att_mask)\n",
    "    \n",
    "validation_masks = []\n",
    "for sent in validation_inputs:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    validation_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Converting to PyTorch Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Our Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (21128, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (3, 768)\n",
      "classifier.bias                                                 (3,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    924.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    924.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    924.    Elapsed: 0:00:23.\n",
      "  Batch   160  of    924.    Elapsed: 0:00:31.\n",
      "  Batch   200  of    924.    Elapsed: 0:00:39.\n",
      "  Batch   240  of    924.    Elapsed: 0:00:46.\n",
      "  Batch   280  of    924.    Elapsed: 0:00:54.\n",
      "  Batch   320  of    924.    Elapsed: 0:01:02.\n",
      "  Batch   360  of    924.    Elapsed: 0:01:10.\n",
      "  Batch   400  of    924.    Elapsed: 0:01:18.\n",
      "  Batch   440  of    924.    Elapsed: 0:01:25.\n",
      "  Batch   480  of    924.    Elapsed: 0:01:33.\n",
      "  Batch   520  of    924.    Elapsed: 0:01:41.\n",
      "  Batch   560  of    924.    Elapsed: 0:01:49.\n",
      "  Batch   600  of    924.    Elapsed: 0:01:57.\n",
      "  Batch   640  of    924.    Elapsed: 0:02:04.\n",
      "  Batch   680  of    924.    Elapsed: 0:02:12.\n",
      "  Batch   720  of    924.    Elapsed: 0:02:20.\n",
      "  Batch   760  of    924.    Elapsed: 0:02:28.\n",
      "  Batch   800  of    924.    Elapsed: 0:02:36.\n",
      "  Batch   840  of    924.    Elapsed: 0:02:43.\n",
      "  Batch   880  of    924.    Elapsed: 0:02:51.\n",
      "  Batch   920  of    924.    Elapsed: 0:02:58.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epcoh took: 0:02:59\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88      2553\n",
      "           1       0.77      0.66      0.71      1232\n",
      "           2       0.82      0.79      0.80      1358\n",
      "\n",
      "    accuracy                           0.82      5143\n",
      "   macro avg       0.81      0.79      0.80      5143\n",
      "weighted avg       0.82      0.82      0.82      5143\n",
      "\n",
      "0.8195300223852311\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    924.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    924.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    924.    Elapsed: 0:00:24.\n",
      "  Batch   160  of    924.    Elapsed: 0:00:31.\n",
      "  Batch   200  of    924.    Elapsed: 0:00:39.\n",
      "  Batch   240  of    924.    Elapsed: 0:00:47.\n",
      "  Batch   280  of    924.    Elapsed: 0:00:55.\n",
      "  Batch   320  of    924.    Elapsed: 0:01:03.\n",
      "  Batch   360  of    924.    Elapsed: 0:01:11.\n",
      "  Batch   400  of    924.    Elapsed: 0:01:19.\n",
      "  Batch   440  of    924.    Elapsed: 0:01:27.\n",
      "  Batch   480  of    924.    Elapsed: 0:01:34.\n",
      "  Batch   520  of    924.    Elapsed: 0:01:42.\n",
      "  Batch   560  of    924.    Elapsed: 0:01:49.\n",
      "  Batch   600  of    924.    Elapsed: 0:01:57.\n",
      "  Batch   640  of    924.    Elapsed: 0:02:05.\n",
      "  Batch   680  of    924.    Elapsed: 0:02:12.\n",
      "  Batch   720  of    924.    Elapsed: 0:02:20.\n",
      "  Batch   760  of    924.    Elapsed: 0:02:27.\n",
      "  Batch   800  of    924.    Elapsed: 0:02:35.\n",
      "  Batch   840  of    924.    Elapsed: 0:02:42.\n",
      "  Batch   880  of    924.    Elapsed: 0:02:50.\n",
      "  Batch   920  of    924.    Elapsed: 0:02:58.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 0:02:58\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88      2553\n",
      "           1       0.74      0.70      0.72      1232\n",
      "           2       0.86      0.72      0.78      1358\n",
      "\n",
      "    accuracy                           0.82      5143\n",
      "   macro avg       0.81      0.78      0.79      5143\n",
      "weighted avg       0.82      0.82      0.82      5143\n",
      "\n",
      "0.8154802954281105\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    924.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    924.    Elapsed: 0:00:15.\n",
      "  Batch   120  of    924.    Elapsed: 0:00:23.\n",
      "  Batch   160  of    924.    Elapsed: 0:00:31.\n",
      "  Batch   200  of    924.    Elapsed: 0:00:39.\n",
      "  Batch   240  of    924.    Elapsed: 0:00:47.\n",
      "  Batch   280  of    924.    Elapsed: 0:00:55.\n",
      "  Batch   320  of    924.    Elapsed: 0:01:03.\n",
      "  Batch   360  of    924.    Elapsed: 0:01:11.\n",
      "  Batch   400  of    924.    Elapsed: 0:01:19.\n",
      "  Batch   440  of    924.    Elapsed: 0:01:27.\n",
      "  Batch   480  of    924.    Elapsed: 0:01:35.\n",
      "  Batch   520  of    924.    Elapsed: 0:01:43.\n",
      "  Batch   560  of    924.    Elapsed: 0:01:51.\n",
      "  Batch   600  of    924.    Elapsed: 0:01:59.\n",
      "  Batch   640  of    924.    Elapsed: 0:02:07.\n",
      "  Batch   680  of    924.    Elapsed: 0:02:15.\n",
      "  Batch   720  of    924.    Elapsed: 0:02:23.\n",
      "  Batch   760  of    924.    Elapsed: 0:02:31.\n",
      "  Batch   800  of    924.    Elapsed: 0:02:39.\n",
      "  Batch   840  of    924.    Elapsed: 0:02:47.\n",
      "  Batch   880  of    924.    Elapsed: 0:02:55.\n",
      "  Batch   920  of    924.    Elapsed: 0:03:03.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:03:04\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86      2553\n",
      "           1       0.70      0.75      0.73      1232\n",
      "           2       0.75      0.84      0.79      1358\n",
      "\n",
      "    accuracy                           0.81      5143\n",
      "   macro avg       0.79      0.80      0.79      5143\n",
      "weighted avg       0.82      0.81      0.81      5143\n",
      "\n",
      "0.8104405607981213\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    924.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    924.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    924.    Elapsed: 0:00:23.\n",
      "  Batch   160  of    924.    Elapsed: 0:00:31.\n",
      "  Batch   200  of    924.    Elapsed: 0:00:39.\n",
      "  Batch   240  of    924.    Elapsed: 0:00:47.\n",
      "  Batch   280  of    924.    Elapsed: 0:00:54.\n",
      "  Batch   320  of    924.    Elapsed: 0:01:02.\n",
      "  Batch   360  of    924.    Elapsed: 0:01:09.\n",
      "  Batch   400  of    924.    Elapsed: 0:01:17.\n",
      "  Batch   440  of    924.    Elapsed: 0:01:24.\n",
      "  Batch   480  of    924.    Elapsed: 0:01:32.\n",
      "  Batch   520  of    924.    Elapsed: 0:01:40.\n",
      "  Batch   560  of    924.    Elapsed: 0:01:47.\n",
      "  Batch   600  of    924.    Elapsed: 0:01:55.\n",
      "  Batch   640  of    924.    Elapsed: 0:02:02.\n",
      "  Batch   680  of    924.    Elapsed: 0:02:10.\n",
      "  Batch   720  of    924.    Elapsed: 0:02:17.\n",
      "  Batch   760  of    924.    Elapsed: 0:02:25.\n",
      "  Batch   800  of    924.    Elapsed: 0:02:33.\n",
      "  Batch   840  of    924.    Elapsed: 0:02:40.\n",
      "  Batch   880  of    924.    Elapsed: 0:02:48.\n",
      "  Batch   920  of    924.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epcoh took: 0:02:56\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87      2553\n",
      "           1       0.70      0.76      0.73      1232\n",
      "           2       0.79      0.80      0.80      1358\n",
      "\n",
      "    accuracy                           0.82      5143\n",
      "   macro avg       0.79      0.80      0.80      5143\n",
      "weighted avg       0.82      0.82      0.82      5143\n",
      "\n",
      "0.8176504943063955\n",
      "  Validation took: 0:00:18\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    max_val_f1 = 0\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    true_labels,predict_labels=[],[]\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        predict_labels.append(np.argmax(logits, axis=1).flatten())\n",
    "        true_labels.append(label_ids.flatten())\n",
    "#         # Calculate the accuracy for this batch of test sentences.\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "#         # Accumulate the total accuracy.\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "#         # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    # Report the final accuracy for this validation run.\n",
    "#     print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    true_labels=[y for x in true_labels for y in x]\n",
    "    predict_labels=[y for x in predict_labels for y in x]    \n",
    "    val_f1=f1_score(true_labels,predict_labels,average='weighted')\n",
    "    print(classification_report(true_labels,predict_labels))\n",
    "    print(val_f1)\n",
    "#     if val_f1 > max_val_f1:\n",
    "#         max_val_f1 = val_f1\n",
    "#         max_val_epoch = epoch_i\n",
    "#         if not os.path.exists('state_dict'):\n",
    "#             os.mkdir('state_dict')\n",
    "#         path = 'state_dict/{0}_val_acc_{1}'.format(model_name.split('/')[1],  round(val_f1, 4))\n",
    "#         torch.save(model.state_dict(), path)\n",
    "#         print('>> saved: {}'.format(path))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f = pd.DataFrame(loss_values)\n",
    "f.columns=['Loss']\n",
    "plt.plot(f.index,f.Loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss of the Model')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Performance On Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cola_public/raw/out_of_domain_dev.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ba882831b758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the dataset into a pandas dataframe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./cola_public/raw/out_of_domain_dev.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence_source'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label_notes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Report the number of sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of test sentences: {:,}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cola_public/raw/out_of_domain_dev.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    \n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
